{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - A Exploration in to Data Analysis\n",
    "\n",
    "## 1 - Introduction\n",
    "\n",
    "This is monologue of my progress through this project.\n",
    "\n",
    "MNIST is the Hello World of neural networks. My aim is to learn as much as possible about neural networks and other data analysis methods by analysing this dataset as well as learning how to best use the features of the high level programming language Python.\n",
    "\n",
    "My approach to this problem will be through the following steps:\n",
    "1. Understand the raw data\n",
    "2. Consider different potential solutions to the problem\n",
    "3. Pick a model\n",
    "4. Process the raw data so it is in a suitable form for the algorithms to work\n",
    "5. Fit/train a model\n",
    "6. Analyse the model and try to find improvements\n",
    "\n",
    "### 1.1 - Understanding the Raw Data\n",
    "\n",
    "The data are in csv files. Each file has 785 columns. For the training data, the first column is the label, that is the digit drawn by the user. The remaining 784 columns make up the images. The images are 28x28 pixels in size and each row represents one whole image. Consequently each image is of relatively low quality and is in greyscale. The testing data is identical but with the label column omitted, hence there are only 784 columns in this file.\n",
    "\n",
    "The images can be reconstructed by taking the first 28 cells as the first row of pixels of the image. The subsequent 28 cells make up the second row of pixels of the image etc.\n",
    "\n",
    "The submission file must have the following format:\n",
    "```\n",
    "ImageId,Label\n",
    "1,0\n",
    "2,0\n",
    "3,0\n",
    "etc.\n",
    "```\n",
    "\n",
    "### 1.2 - Potential Solutions\n",
    "\n",
    "One popular solution is to use a neural network to classify the digits. This is something I will explore using Tensorflow. If this is successful I may then look to improve this training process with regards to speed and efficiency by training the model in C/C++ on my GPU.\n",
    "\n",
    "This problem would also lend itself well to using some sort of clustering algorithm. Further research and implentation in to k-means clustering algorithms and the algorithms I have learned at university will be considered.\n",
    "\n",
    "A classification regression tree may also work here.\n",
    "\n",
    "Finally I am aware of support-vector machines. I am not entirely sure what these are hence more research is required, but that is another method that should be explored.\n",
    "\n",
    "## 2 - Neural Network Model\n",
    "\n",
    "### 2.0 - Introduction\n",
    "\n",
    "The neural network model requires an initial layer with one node per pixel. In this case, that is 28^2 input nodes. The output layer requires as many nodes as there are classes. This final layer usually uses a softmax activation function on each of the nodes. This converts each of the nodes to a value between 0 and 1 such that the sum of all the values sums to 1. This means the output can be interpreted as a probability of the image beloging to each particular class.\n",
    "\n",
    "The difficult part of neural networks is filling in the hidden layers. A single layer is only capable of linear regression. However, deeper networks and other special layers such as convolutional layers allow for a model to pick different features. Hidden layers will often use the rectified linear unit (ReLU) activation function on each node. This is a very natural interpretation of a neuron. \n",
    "\n",
    "The aim here is to learn how these layers can be combined to create a good model and this shall be done through hands on experimentation and through research.\n",
    "\n",
    "### 2.1 - Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # For the jupyter notebook\n"
     ]
    }
   ],
   "source": [
    "# For the matplotljupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # For linear algebra\n",
    "import pandas as pd # For data structures and easy file handling\n",
    "import matplotlib.pyplot as plt # For plotting graphs easily\n",
    "import tensorflow as tf # For neural networks using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shall now read the train and test data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train = pd.read_csv('data/train.csv')\n",
    "raw_data_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(type(raw_data_test))\n",
    "\n",
    "print(raw_data_train.head())\n",
    "\n",
    "print('Training dimensions', raw_data_train.shape)\n",
    "print('Testing dimensions', raw_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently the train dataset has 42000 rows and the test dataset has 28000 rows. So there is plenty of data to be working with!\n",
    "\n",
    "First, I need to preprocess the data so it is in a usuable form. The block below coverts the raw data stored as pd.DataFrames and converts it to np.arrays. This can be thought of as reconstructing the original image. This will be flattened after any convolutional layers (if present) in the neural network model anyway (sending it back to a 1D vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-05cec0ca626b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_data_train' is not defined"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def prep_raw_data(raw_data):\n",
    "    \"\"\"Splits the training data in to training and validation datasets.\n",
    "    \"\"\"\n",
    "    y = raw_data.label.values\n",
    "    out_y = tf.keras.utils.to_categorical(y=y, num_classes=num_classes)\n",
    "    x = raw_data.values[:, 1:]\n",
    "    return out_y, reshape_data(x)\n",
    "\n",
    "def reshape_data(array):\n",
    "    \"\"\"Returns n x 28 x 28 numpy array that has been standardised.\n",
    "    \"\"\"\n",
    "    if isinstance(array, pd.DataFrame):\n",
    "        #assume array is the testing data and convert it to a numpy array\n",
    "        array = array.values[:,:]\n",
    "        \n",
    "    return array.reshape(array.shape[0], 28, 28)/255\n",
    "\n",
    "y_train, x_train = prep_raw_data(raw_data_train)\n",
    "x_test = reshape_data(raw_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to show that the preprocessing has worked, here is a plot of a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[123])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - First Model\n",
    "\n",
    "Here I shall bulid a model starting with a very basic architecture and try to improve the model iteratively by adding different layer types and combinations.\n",
    "\n",
    "First I will start with a hidden layer with 100 nodes and ReLU as the activation function. To compile to model I will just use stochastic gradient descent (SGD) at first and I shall explore other algorithms some other time. [Click here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) to view the optimiser algorithms built in to TF. I shall set the loss function to categorical crossentropy. WHY?!?!?! The metric is added to simply show the accuracy during training, it is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model_1 = tf.keras.models.Sequential()\n",
    "basic_model_1.add(tf.keras.layers.Flatten())\n",
    "basic_model_1.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "basic_model_1.add(tf.keras.layers.Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "basic_model_1.compile(optimizer = 'SGD', \n",
    "                    loss = tf.keras.losses.categorical_crossentropy,\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "basic_model_1.fit(x = x_train, y = y_train)\n",
    "\n",
    "predictions_1 = np.argmax(basic_model_1.predict(x = x_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'ImageId': range(1,28001), 'Label': predictions_1})\n",
    "output.to_csv('Submissions.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to kaggle, this model achieved a 88.5% accuracy! Not bad. However this puts me 2906th of 3180 submissions... that's the 92nd percentile (at the time of writing, June 2020). \n",
    "\n",
    "The next step is to explore methods that will improve the accuracy of results. I should also change the way I check my accuracy of my models as uploading to Kaggle every time is slow.\n",
    "\n",
    "To do this I will split the training data in to a training and validation set. I can hard code this by physically splitting the data or I can use `validation_split = 0.2` to let TF decide which data points should be set aside for validation during the fit. See below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model_2 = tf.keras.models.Sequential()\n",
    "basic_model_2.add(tf.keras.layers.Flatten())\n",
    "basic_model_2.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "basic_model_2.add(tf.keras.layers.Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "basic_model_2.compile(optimizer = 'SGD', \n",
    "                    loss = tf.keras.losses.categorical_crossentropy,\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "basic_model_2.fit(x = x_train, \n",
    "                y = y_train,\n",
    "                validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy can now be seen from the output above. This value changes everytime the code is run as different parts of the data are reserved each time for the validation.\n",
    "\n",
    "Now would be a good time to infer each of the output terms above.\n",
    "\n",
    "\n",
    "\n",
    "Further topics to explore include:\n",
    "- Activation functions\n",
    "- Loss Functions\n",
    "- Epochs\n",
    "- Batch sizes\n",
    "- Optimisation Methods\n",
    "- Node count per layer\n",
    "- Number of layers\n",
    "- Dropout\n",
    "- Different types of layers\n",
    "- Convolutional Layers\n",
    "- Stride length\n",
    "- Data Augmentation (not really applicable here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
